<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
        margin: 0;
		background-color: #f5f9ff;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: left;
		align-items: center;
        background-color: #ffffff;
	}
	.main-content-block {
		width: 70%;
    max-width: 1100px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%;
			max-width: 130px;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			font-size: 14px;
			width: 25%;
			max-width: 256px;
			position: relative;
			text-align: left;
			padding: 10px;
	}

	img {
			max-width: 100%;
			height: auto;
			display: block;
			margin: auto;
	}

	a:link,a:visited {
		color: #0e7862;
		text-decoration: none;
	}
	a:hover {
		color: #24b597;
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px);
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

	div.hypothesis {
		width: 90%;
		background-color: #f0f7f5;
		border: 1px solid #0e7862;
		border-radius: 10px;
		font-size: 16px;
		text-align: left;
		margin: auto;
		padding: 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: auto;
  }

	table.results {
		width: 100%;
		border-collapse: collapse;
		margin: 15px 0;
		font-size: 14px;
	}
	table.results th, table.results td {
		border: 1px solid #DDD;
		padding: 8px;
		text-align: center;
	}
	table.results th {
		background-color: #f0f7f5;
	}

	.figure-caption {
		font-size: 13px;
		color: #555;
		text-align: center;
		margin-top: 0px;
		font-style: italic;
        display: block;  
	}

    .figure-caption-right {
    font-size: 13px;
    color: #555;
    text-align: left;
    margin-top: 0px;
    font-style: normal;
    display: block;
    }

	.equation-box {
		background-color: #fafafa;
		border: 1px solid #eee;
		border-radius: 5px;
		padding: 15px;
		margin: 15px auto;
		text-align: center;
		font-family: "Times New Roman", Times, serif;
		font-size: 16px;
	}

	.insight-box {
		background-color: #fff9e6;
		border-left: 4px solid #cc9900;
		padding: 12px 16px;
		margin: 15px 0;
		font-size: 14px;
	}
    .figure-legend {
    background-color: #f7f9ff;
    border-left: 3px solid #999;
    border-radius: 4px;
    padding: 8px 10px;
    font-size: 13px;
    color: #555;
    }

    .figure-legend-title {
    font-weight: bold;
    margin-bottom: 4px;
    }

    .figure-legend-list {
    margin: 4px 0 6px 16px;
    padding: 0;
    }

    .figure-legend-list li {
    margin-bottom: 2px;
    }

    .section-divider {
    border-top: 1px solid #DDD;
    margin: 24px 0 16px 0;
    }

    .popout {
    background-color: #eef4ff;
    border: 1px solid #d1ddf5;
    border-left: 4px solid #3a5ea8;
    border-radius: 6px;
    padding: 12px 14px;
    margin: 15px 0;
    }

    .popout-title {
    font-weight: bold;
    font-size: 15px;
    color: #2b4c8c;
    margin-bottom: 8px;
    }

    .popout pre {
    margin: 0;
    font-size: 14px;
    line-height: 1.5;
    font-family: "Courier New", Courier, monospace;
    white-space: pre-wrap;
    }

</style>

	<title>Titan-LLaMA: Neural Memory Adapters for Continual Learning</title>
	<meta property="og:title" content="Titan-LLaMA: Neural Memory Adapters for Continual Learning" />
	<meta charset="UTF-8">
</head>

<body>

	<!-- HEADER -->
	<div class="content-margin-container">
			<div class="margin-left-block">
			</div>
	    <div class="main-content-block">
					<table class="header" align=left>
							<tr>
								<td colspan=4>
									<span style="font-size: 28px; font-family: 'Courier New', Courier, monospace;">Titan-LLaMA: Neural Memory Adapters for Continual Learning</span>
								</td>
							</tr>
							<tr>
									<td align=left>
											<span style="font-size:17px"><a href="#">Sarah Pan</a></span>
									</td>
									<td align=left>
											<span style="font-size:17px"><a href="#">Sarah Dufays</a></span>
									</td>
							<tr>
								<td colspan=4 align=left><span style="font-size:18px">Final Project for 6.7960, Massachusetts Institute of Technology</span></td>
							</tr>
					</table>
				</div>
				<div class="margin-right-block">
				</div>
	</div>

	<!-- TABLE OF CONTENTS + HERO IMAGE -->
	<div class="content-margin-container" id="intro">
			<div class="margin-left-block">
        <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
            <b style="font-size:16px">Outline</b><br><br>
            <a href="#motivation">Motivation</a><br><br>
            <a href="#background">Background</a><br><br>
            <a href="#methods">Methods</a><br><br>
            <a href="#experiments">Experiments</a><br><br>
            <a href="#results">Results</a><br><br>
            <a href="#limitations">Limitations and Future Work</a><br><br>
            <a href="#references">References</a><br><br>
        </div>
			</div>
	    <div class="main-content-block">
	    </div>
	</div>

    <!-- MOTIVATION -->
    <div class="content-margin-container" id="motivation">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
        <h2>Motivation</h2>

        In NLP, we already have a bunch of very strong, off-the-shelf LLMs, but their
        "memory" cost grows quadratically with sequence length. Most open-source fixes
        are linear-time hacks&mdash;sliding windows, RAG, caching tricks, etc&mdash;rather than a
        true learned state. As we discuss in the background, these approaches keep the
        model stateless and outsource memory to context or retrieval.<br><br>

        Titans stood out to us because it is a very unique approach that does the opposite: the state is a learned
        architecture, not a transient context vector that disappears when the window
        moves on. We wanted to extend that idea and build modular Titan adapters that
        augment a pre-trained LLM whose long-range memory we’ve deliberately incapacitated.<br><br>

        No one has done this before: combining Titans-style neural
        memory with a frozen backbone as a drop-in "memory cartridge". The result is
        an architecture that (i) generates tokens much faster as generation length
        grows and (ii) can be fine-tuned by swapping in task-specific memory adapters
        for downstream domains.<br><br>
        
        Architecturally, this buys us a different scaling regime.
        Full-attention LLaMA pays a quadratic price in sequence length, so tokens/sec
        inevitably drop as generations get longer. Once we segment attention, each
        token only attends within a fixed-size window, so generation cost is tied to
        segment size rather than total sequence length. With a well-optimized NMM,
        a segmented-with-memory model should overtake full attention beyond a few
        segments because the longer you generate, the larger the relative speedup.<br><br>

        <img src="./images/throughput_vs_generation.png" width="750px"/>
        <p class="figure-caption">
          Figure 1: Token throughput vs. generation length. Full-attention LLaMA slows
          down as sequences grow, while segmented attention keeps throughput roughly
          flat; our segmented+NMM model should eventually beat full attention once the
          memory kernels are optimized.
        </p>
      </div>
      <div class="margin-right-block">
        <span class="figure-caption-right">
          Note that in our
        prototype the throughput curve still lags slightly behind full
        attention because the NMM kernels are under-optimized and add extra
        intermediate computation, but that is a systems issue, not a limitation of
        the design.
        </span>
      </div>
    </div>

     <!-- BACKGROUND divider before Titans -->
    <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <div class="section-divider"></div>
    </div>
    <div class="margin-right-block"></div>
    </div>

    <!-- BACKGROUND: part 0  -->
    <div class="content-margin-container" id="background">
    <div class="margin-left-block">
    </div>

    <div class="main-content-block">
        <h2>Background</h2>

        Prior work on adapting language models falls into two categories.<br><br>
        <b>Memory and state.</b> RNNs and LSTMs compress history into a fixed-size hidden vector but forget distant information. SSMs <a href="#ref_3">[3]</a> revisit this idea by improving stability and efficiency with linear dynamics. Transformers on the other hand store everything explicitly via attention, relying on a KV cache that grows with sequence length. This means every new token attends over all previous ones, which gives precise access to the past but with quadratic cost in the context length.<br><br>
    </div>
    <div class="margin-right-block">
        <span class="figure-caption-right">
        This illustrates a core tradeoff in ML: recurrent models compress history into a fixed-size state but struggle with very long contexts, while transformers store everything explicitly and become slow and memory-hungry.
        </span>
    </div>
    </div>

    <!-- BACKGROUND: part 1 (Memory and state + side note) -->
    <div class="content-margin-container">
    <div class="margin-left-block">
    </div>

    <div class="main-content-block">
        <b>Parameter-efficient adapters.</b> Full fine-tuning is expensive and risks catastrophic forgetting. LoRA <a href="#ref_1">[1]</a> instead adds trainable low-rank factors to frozen weight matrices, reducing trainable parameters by orders of magnitude while preserving expressivity. Cartridges <a href="#ref_2">[2]</a> go further, treating the KV cache as an adapter: learned virtual KV vectors are prepended to the context so the frozen model behaves as if it had read a particular corpus. Both keep backbone weights untouched.<br><br>
    </div>

    <div class="margin-right-block">
    </div>
    </div>

    <!-- BACKGROUND divider -->
    <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <div class="section-divider"></div>
    </div>
    <div class="margin-right-block"></div>
    </div>

    <!-- BACKGROUND: Titans -->
    <div class="content-margin-container">
    <div class="margin-left-block">
    </div>

    <div class="main-content-block">
        <b>Titans: deep neural memory as state.</b> Titans <a href="#ref_4">[4]</a>, introduced by Behrouz et al. [Google Research] in 2024, combines transformer-level accuracy with RNN-like speeds by maintaining a deep neural memory alongside attention. Titans introduces three fundamental innovations:
				
		<br><br><b>1. True Test-Time Memory.</b> Instead of just expanding context windows or adding retrieval mechanisms, Titans learns to memorize at test time. The neural long-term memory module learns what to remember while running, gets smarter with every interaction, and maintains context across extended use.<br><br>

    </div>

    <div class="margin-right-block">
    </div>
    </div>

    <!-- BACKGROUND: part 2 (Surprise-based memory + Momentum/Forgetting) -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>

      <div class="main-content-block">
        <b>2. Surprise-Based Memory.</b> Rather than treating every token in the context as equally important, Titans uses a learned "surprise" score to decide what is worth remembering and therefore what actually gets written into its neural memory.
        We can compare this to how in human psychology, we quickly forget routine events but remember unexpected, surprising, or highly emotional events.

        <div class="insight-box">
          <b>Low surprise:</b> If the model is reading the 6.7960 course page and its memory is expecting yet another “PSET released” announcement, the gradient (surprise) is low so it can safely skip memorizing this in permanent storage.<br><br>
          <b>High surprise:</b> If the same course page suddenly says “All remaining PSETs are cancelled; everyone gets an A! =)" the gradient will be very high, indicating that this is important and should be prioritized for permanent storage.
        </div>

        The model uses this internal error signal (the gradient) as a mathematical equivalent of saying "Pay attention to this!" This allows selective memory updates with only the most important information.<br><br>

        Titans considers both "momentary surprise" (current input) and "past surprise" (recent context flow) to make sure that relevant subsequent information is captured even if individual tokens aren't surprising.</li>
    </div>

      <div class="margin-right-block">
      </div>
    </div>

    <!-- SURPRISE-->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>

      <div class="main-content-block">
        <img src="./images/surprise_formula.png" width="450px"/>
        <p class="figure-caption">
          Figure 2: Surprise update rule. Memory M<sub>t</sub> = M<sub>t-1</sub> + S<sub>t</sub>, where S<sub>t</sub> mixes momentum from past surprise with the current prediction-error gradient.
        </p>
        <br><b>3. Adaptive Forgetting.</b> To better manage the memory's limited capacity, Titans adds an adaptive forgetting gate (similar to the gating mechanism in modern RNNs) that scales the previous memory before adding the new surprise update, so the model can softly decay stale information or, in the extreme, wipe the memory entirely.<br><br><br><br>
      </div>

      <div class="margin-right-block">
      </div>
    </div>

     <!-- BACKGROUND divider before Titans -->
    <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <div class="section-divider"></div>
    </div>
    <div class="margin-right-block"></div>
    </div>

    <!-- BACKGROUND: part 3 (Adaptive Forgetting + three memory pieces) -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>

      <div class="main-content-block">        
        <b>Titans factor memory into three pieces:</b>
        <ul>
          <li><b>Short-term memory:</b> localized attention windows over recent tokens</li>
          <li><b>Long-term memory:</b> a small MLP that maps keys to values, updated online via the surprise loss</li>
          <li><b>Persistent memory:</b> task-level parameters independent of the input sequence</li>
        </ul>
      </div>

      <div class="margin-right-block">
        <span class="figure-caption-right">
            Note that Ali Behrouz (Titans author) mentions on the Cognitive Revolution podcast <a href="#ref_15">[15]</a> that persistent memory is mostly there for a human-memory analogy and isn’t really needed in practice, so we ignore it in our implementation and just add the short and long-term memory pieces.
        </span>
      </div>
    </div>


    <!-- TITANS OVERVIEW: Figure 3 + legend under image -->
    <div class="content-margin-container">
    <div class="margin-left-block">
    </div>

    <div class="main-content-block">
        <img src="./images/titans_overview.png" width="1000px"/>

        <p class="figure-caption">
        Figure 3: Titans MAC architecture.
        </p>

        <div class="figure-legend">
        <ul class="figure-legend-list">
            <li><b>Top band:</b> long-term contextual memory (Neural Memory)</li>
            <li><b>Middle band:</b> short-term sequence + attention (Core)</li>
            <li><b>Bottom band:</b> persistent task-specific parameters (Persistent Memory)</li>
        </ul>
        <div>
            Neural memory stores a compressed summary of past tokens that is
            retrieved, updated, and injected back into attention over the
            current sequence.
        </div>
        </div>
    </div>

    <div class="margin-right-block">
        <!-- empty to keep layout consistent -->
    </div>
    </div>

    <!-- BACKGROUND: part 4 (wrap-up text) -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>

      <div class="main-content-block">
        <br>In summary, memory is not a vector or cache but a trainable module whose weights are the state—the model learns and adapts dynamically as new data arrives.<br><br>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

     <!-- BACKGROUND divider before Titans -->
    <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <div class="section-divider"></div>
    </div>
    <div class="margin-right-block"></div>
    </div>

	<!-- METHODS -->
    <div class="content-margin-container" id="methods">
    <div class="margin-left-block">
    </div>

    <div class="main-content-block">
        <h2>Methods</h2>

        <b>Our perspective:</b> We combine these views by attaching Titan-style memory
        to a frozen LLaMA backbone and treating it as a cartridge-style adapter.
        The backbone stays fixed; only the Titan memory continues learning.
    </div>

    <div class="margin-right-block">
    </div>
    </div>

	<div class="content-margin-container">
			<div class="margin-left-block">
			</div>
	    <div class="main-content-block">

    <img src="./images/nmm_bridge.png" width=700px/>
    <i class="figure-caption">Figure 4: The NMM (green) injects cross-segment attention entries, allowing information retrieval across segment boundaries without full-sequence attention.</i>

    <br>The next step of the process is to augment the localized attention calculation with "long-term memory"
    that is retrieved from the NMM. The NMM will try to recall what happened at the previous  
    local attention window. <br><br>

				
	    </div>
	    <div class="margin-right-block">
	    </div>
	</div>

	<div class="content-margin-container">
			<div class="margin-left-block">
			</div>
	    <div class="main-content-block">
				<img src="./images/temporal_loop.png" width=700px/>

                <i class="figure-caption">Figure 5: As chunks c1–c5 progress, the NMM is queried then updated based on prediction error, persisting information across the full sequence.</i>

				<br>
				It might be helpful to "flip" the time axis when thinking about the fast update process. If we imagine
				each localized attention segment to be its own element in a sequence, this process closely resembles that
				of an LSTM or RNN. However, instead of learning weights that modulate previous hidden cell states, the hidden
				state itself is parameterized as a deep, learned architecture. 

				By storing the gradients for the surprisal-based loss, our neural memory module is essentially trained
				to remember all the prior attention outputs.<br><br> </div>

	    <div class="margin-right-block">
	    </div>
	</div>

	<div class="content-margin-container">
			<div class="margin-left-block">
			</div>
	    <div class="main-content-block">
				<img src="./images/full_architecture.png" width=700px/>
                <i class="figure-caption">Figure 6: Purple: frozen LLaMA. Green: trainable NMM + fast gradients. <br> Only the memory "cartridge" adapts. The backbone is completely frozen: only ~2% of parameters are trainable.</i><br><br>


				Finally, we propagate the language modeling loss over the weights of the NMMs and actually perform
				weight updates. It's important to note here that the rest of the weights in the pre-trained backbone
				are frozen and no updates are made to them. We can think of these "slow updates" as teaching the NMMs
				<em>how</em> to recall best.<br><br>
	    </div>
	    <div class="margin-right-block">
	    </div>
	</div>

	<div class="content-margin-container">
			<div class="margin-left-block">
			</div>
	    <div class="main-content-block">

				<b>Attention Distillation Loss</b><br>

				In practice, we noticed that the pure language modeling objective was really easy for NMMs to master.
				And in fact, we do much better than our out-of-the-box LLM when evaluating next token prediction purely on a token accuracy /
				 perplexity basis. However, when it came to the challenging language benchmarks that pre-trained LLMs excel at,
				solely using the language modeling loss was insufficient. <br></br>

				In our setting, we want the NMM to first recover baseline capabilities of the backbone LLM. We supervise the outputs of the Titan-agumented 
				layers with that from our pre-trained backbone. In effect, we implicitly distill the expressivity from our pre-trained "teacher attention" into our
				adapter's recollection strategy only using a single additional forward pass at train-time. <br></br>


				<div class="popout">
					<pre>Calculating Attention Distillation Loss: </pre>
					<pre>   1. Compute normal titan-llama forward pass</pre>
					<pre>        a. Collect hidden states, these are “student” latents</pre>
					<pre>   2. Do a forward pass over the backbone, without segmented attn</pre>
					<pre>        a. Collect hidden states, these are “teacher” latents</pre>
					<pre>   3. Compute MSE over these latents, scale, and add to LM loss</pre>
					
				</div>
				
	    </div>
	</div>

     <!-- BACKGROUND divider before Titans -->
    <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <div class="section-divider"></div>
    </div>
    <div class="margin-right-block"></div>
    </div>

	<!-- EXPERIMENTS -->
	<div class="content-margin-container" id="experiments">
			<div class="margin-left-block">
			</div>
	    <div class="main-content-block">
				<h2>Experiments</h2>

				<div class="hypothesis">
                    <b>Hypothesis:</b> A Titans-style neural memory, used as an adapter on top of a frozen LLM backbone, can <br></br>
						&emsp;(1) recover backbone LLM capabilities  <br></br>
						&emsp;(2) be fine-tuned to specialize in new domains <br></br>
				</div><br>

				<b>Experiment 1: Can Titan-LLaMA recover base LM abilities?</b><br><br>

				We test whether the NMM can compensate for the information lost when replacing full attention with cheap segmented attention.<br><br>

				<i>1a. Language Modeling:</i> Train the Titan-LLaMA adapter on 1B tokens from SlimPajama-627B and FineWeb-EDU. Report validation perplexity and token accuracy.<br><br>

				<i>1b. NLP Benchmarks:</i> Evaluate on Winogrande <a href="#ref_7">[7]</a>, BoolQ <a href="#ref_8">[8]</a>, CommonsenseQA <a href="#ref_9">[9]</a>, DROP <a href="#ref_10">[10]</a>, SQuAD <a href="#ref_11">[11]</a>, and PubMedQA <a href="#ref_12">[12]</a>. Compare against frozen LLaMA (full attention) and segmented-only (ablation).<br><br>

				<b>Experiment 2: Domain Adaptation</b><br><br>

				Can we train domain-specific NMMs that specialize to new areas?<br>
				<ul>
					<li><b>Biomed:</b> PubMed corpus, PubMedQA <a href="#ref_12">[12]</a></li>
					<li><b>Legal:</b> LexGLUE benchmark <a href="#ref_13">[13]</a></li>
					<li><b>Math:</b> AQUA-RAT word problems <a href="#ref_14">[14]</a></li>
				</ul>
	    </div>
	    <div class="margin-right-block">
				We test both <i>recovery</i> (can NMM fix segmentation?) and <i>adaptation</i> (can NMM specialize?).
	    </div>
	</div>

     <!-- BACKGROUND divider before Titans -->
    <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <div class="section-divider"></div>
    </div>
    <div class="margin-right-block"></div>
    </div>

	<!-- RESULTS -->
	<div class="content-margin-container" id="results">
			<div class="margin-left-block">
			</div>
	    <div class="main-content-block">
				<h2>Results</h2>

				<h1>Experimental Setup</h1>

                We use <a href="https://huggingface.co/meta-llama/Llama-3.1-8B">Llama-3.1-8b</a> as our pre-trained LLM backbone. For the NMMs, we attach
                2-layer MLPs, with hidden dimension 2048 onto layers 4, 8, 12, 16, and 20. We use an attention segment length half the size of the data sequence
                length, a neural memory segment length of 64, and a neural memory batch size of 64. <br></br>

                For pre-training data, we use a 1b-token mixture of <a href="https://huggingface.co/datasets/cerebras/SlimPajama-627B">Slim Pajama</a> and
                <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">FineWeb-EDU</a>. We fine-tune on the training sets of 
                <a href="https://huggingface.co/datasets/qiaojin/PubMedQA">PubMedQA</a>, <a href="https://huggingface.co/datasets/casehold/casehold">CaseHOLD</a>,
                <a href="https://huggingface.co/datasets/deepmind/aqua_rat">AQUA-RAT</a>. <br></br>

                We find that higher neural learning rates (on the order of 1e-3) are helpful to performance. Gradient updates are performed
                with a learning rate of 3e-4 with a linear warmup and cosine annealing. Further, we weigh the distillation loss at 0.6 relative
                to language modeling loss. Due to the scope of the project, we were unable to thoroughly vet our hyperparameter setup--further
                work will explore annealing distillation loss weight and more tuning. <br><br>

                <h1>Experiment 1 Results</h1>
				

			    We verified that imparing attention masks led to catastrophic results. Performance of Llama-Titan exceeded
                that of the frozen backbone on next-token prediction / language modeling.<br><br>

				<table class="results">
					<tr>
						<th>Model</th>
						<th>Token Accuracy</th>
						<th>Perplexity</th>
					</tr>
					<tr>
						<td>Full-Attention LLaMA</td>
						<td>55.2%</td>
						<td>7.26</td>
					</tr>
					<tr>
						<td>Segmented-only (no NMM)</td>
						<td>0%</td>
						<td>1,516,042</td>
					</tr>
					<tr>
						<td><b>Titan-LLaMA (Segmented + NMM)</b></td>
						<td><b>100%</b></td>
						<td><b>~1.0</b></td>
					</tr>
				</table>

				The NMM not only recovers but actually exceeds baseline language modeling metrics—achieving 100% token accuracy vs. 55.2% for the original model. This suggests the NMM learns an effective compression of the training distribution.<br><br>

			    Despite superior LM metrics, downstream benchmarks barely improve until we add the attention distillation loss and obtain the following superior results:<br><br>

				<table class="results">
					<tr>
						<th>Benchmark</th>
						<th>Full-Attn LLaMA</th>
						<th>Segmented-only</th>
						<th>Titan-LLaMA (LM loss)</th>
					</tr>
					<tr>
						<td>Winogrande</td>
						<td>60.5%</td>
						<td>48.4%</td>
						<td>52.3% (+3.9%)</td>
					</tr>
					<tr>
						<td>BoolQ</td>
						<td>75.0%</td>
						<td>43%</td>
						<td>63% (+20%)</td>
					</tr>
					<tr>
						<td>CommonsenseQA</td>
						<td>75.0%</td>
						<td>13%</td>
						<td>18% (+5%)</td>
					</tr>
					<tr>
						<td>DROP</td>
						<td>59.5%</td>
						<td>0%</td>
						<td>7.4% (+7.4%)</td>
					</tr>
                    <tr>
						<td>SquadV2</td>
						<td>77%</td>
						<td>0%</td>
						<td>6.25% (+6.25%)</td>
					</tr>
				</table>

				<!-- Distillation effect visualization -->
				<div style="background-color: #f9f9f9; border: 1px solid #ddd; border-radius: 8px; padding: 20px; margin: 20px 0;">
					<b>Effect of Distillation Weight on Performance</b><br><br>
					<svg width="500" height="200" style="display: block; margin: auto;">
						<!-- Axes -->
						<line x1="60" y1="160" x2="460" y2="160" stroke="#333" stroke-width="1"/>
						<line x1="60" y1="160" x2="60" y2="20" stroke="#333" stroke-width="1"/>
						
						<!-- X-axis labels -->
						<text x="60" y="180" font-size="11" text-anchor="middle">0</text>
						<text x="160" y="180" font-size="11" text-anchor="middle">0.3</text>
						<text x="260" y="180" font-size="11" text-anchor="middle">0.5</text>
						<text x="360" y="180" font-size="11" text-anchor="middle">0.7</text>
						<text x="460" y="180" font-size="11" text-anchor="middle">1.0</text>
						<text x="260" y="198" font-size="12" text-anchor="middle">Distillation Weight (λ)</text>
						
						<!-- Y-axis labels -->
						<text x="50" y="160" font-size="10" text-anchor="end">40%</text>
						<text x="50" y="115" font-size="10" text-anchor="end">50%</text>
						<text x="50" y="70" font-size="10" text-anchor="end">60%</text>
						<text x="50" y="25" font-size="10" text-anchor="end">70%</text>
						<text x="25" y="95" font-size="12" text-anchor="middle" transform="rotate(-90, 25, 95)">Accuracy</text>
						
						<!-- Grid lines -->
						<line x1="60" y1="115" x2="460" y2="115" stroke="#eee" stroke-width="1"/>
						<line x1="60" y1="70" x2="460" y2="70" stroke="#eee" stroke-width="1"/>
						
						<!-- BoolQ line (green) -->
						<polyline points="60,151 160,133 260,115 360,97 460,88" fill="none" stroke="#2e7d32" stroke-width="2"/>
						<circle cx="60" cy="151" r="4" fill="#2e7d32"/>
						<circle cx="160" cy="133" r="4" fill="#2e7d32"/>
						<circle cx="260" cy="115" r="4" fill="#2e7d32"/>
						<circle cx="360" cy="97" r="4" fill="#2e7d32"/>
						<circle cx="460" cy="88" r="4" fill="#2e7d32"/>
						
						<!-- Winogrande line (blue) -->
						<polyline points="60,142 160,133 260,124 360,106 460,97" fill="none" stroke="#1565c0" stroke-width="2"/>
						<circle cx="60" cy="142" r="4" fill="#1565c0"/>
						<circle cx="160" cy="133" r="4" fill="#1565c0"/>
						<circle cx="260" cy="124" r="4" fill="#1565c0"/>
						<circle cx="360" cy="106" r="4" fill="#1565c0"/>
						<circle cx="460" cy="97" r="4" fill="#1565c0"/>
						
						<!-- Legend -->
						<rect x="320" y="30" width="12" height="12" fill="#2e7d32"/>
						<text x="338" y="40" font-size="11">BoolQ</text>
						<rect x="390" y="30" width="12" height="12" fill="#1565c0"/>
						<text x="408" y="40" font-size="11">Winogrande</text>
					</svg>
					<p style="text-align: center; font-size: 13px; color: #666; margin-top: 10px;">Increasing distillation weight improves benchmark performance while LM metrics remain stable.</p>
				</div>

				With distillation, token accuracy drops to ~60% (vs 100% without). The model sacrifices some LM performance to learn richer representationss. This tells us that benchmark performance depends on representation quality, not just next-token prediction accuracy.
	    
                <br><br>
                <h1>Experiment 2 Results</h1>
                <b>Domain-specific fine-tuning.</b> We fine-tuned Titan-LLaMA on 3 domain-specific tasks:<br><br>

				<table class="results">
					<tr>
						<th>Benchmark</th>
						<th>Full-Attn LLaMA</th>
						<th>Segmented-only</th>
						<th>Titan-LLaMA (fine-tuned)</th>
					</tr>
					<tr>
						<td>PubMedQA (Biomed)</td>
						<td>58%</td>
						<td>46%</td>
						<td>55%</td>
					</tr>
					<tr>
						<td>CaseHOLD (Legal)</td>
						<td>33%</td>
						<td>19%</td>
						<td>23%</td>
					</tr>
					<tr>
						<td>AQUA-RAT (Math)</td>
						<td>28%</td>
						<td>22%</td>
						<td>24.8%</td>
					</tr>
				</table>

                <!-- Domain fine-tuning visualization -->
				<div style="background-color: #f9f9f9; border: 1px solid #ddd; border-radius: 8px; padding: 20px; margin: 20px 0;">
					<b>Domain Fine-Tuning Visualization</b><br><br>
					<svg width="520" height="240" style="display: block; margin: auto;">
						<!-- Axes -->
						<line x1="80" y1="190" x2="480" y2="190" stroke="#333" stroke-width="1"/>
						<line x1="80" y1="190" x2="80" y2="30" stroke="#333" stroke-width="1"/>
						
						<!-- Y-axis labels -->
						<text x="70" y="190" font-size="10" text-anchor="end">0%</text>
						<text x="70" y="150" font-size="10" text-anchor="end">20%</text>
						<text x="70" y="110" font-size="10" text-anchor="end">40%</text>
						<text x="70" y="70" font-size="10" text-anchor="end">60%</text>
						<text x="30" y="115" font-size="12" text-anchor="middle" transform="rotate(-90, 30, 115)">Accuracy</text>
						
						<!-- Grid lines -->
						<line x1="80" y1="150" x2="480" y2="150" stroke="#eee" stroke-width="1"/>
						<line x1="80" y1="110" x2="480" y2="110" stroke="#eee" stroke-width="1"/>
						<line x1="80" y1="70" x2="480" y2="70" stroke="#eee" stroke-width="1"/>
						
						<!-- PubMedQA bars (58%, 46%, 55%) -->
						<rect x="100" y="74" width="28" height="116" fill="#6b6b8d" opacity="0.8"/>
						<rect x="132" y="98" width="28" height="92" fill="#dc3545" opacity="0.7"/>
						<rect x="164" y="80" width="28" height="110" fill="#28a745" opacity="0.8"/>
						<text x="147" y="205" font-size="10" text-anchor="middle">PubMedQA</text>
						
						<!-- CaseHOLD bars (33%, 19%, 23%) -->
						<rect x="240" y="124" width="28" height="66" fill="#6b6b8d" opacity="0.8"/>
						<rect x="272" y="152" width="28" height="38" fill="#dc3545" opacity="0.7"/>
						<rect x="304" y="144" width="28" height="46" fill="#28a745" opacity="0.8"/>
						<text x="287" y="205" font-size="10" text-anchor="middle">CaseHOLD</text>
						
						<!-- AQUA-RAT bars (28%, 22%, 24.8%) -->
						<rect x="380" y="134" width="28" height="56" fill="#6b6b8d" opacity="0.8"/>
						<rect x="412" y="146" width="28" height="44" fill="#dc3545" opacity="0.7"/>
						<rect x="444" y="140" width="28" height="50" fill="#28a745" opacity="0.8"/>
						<text x="427" y="205" font-size="10" text-anchor="middle">AQUA-RAT</text>
						
						<!-- Legend -->
						<rect x="120" y="15" width="12" height="12" fill="#6b6b8d" opacity="0.8"/>
						<text x="138" y="25" font-size="10">Full-Attn LLaMA</text>
						<rect x="240" y="15" width="12" height="12" fill="#dc3545" opacity="0.7"/>
						<text x="258" y="25" font-size="10">Segmented-only</text>
						<rect x="360" y="15" width="12" height="12" fill="#28a745" opacity="0.8"/>
						<text x="378" y="25" font-size="10">Titan-LLaMA</text>
						
						<!-- Value labels on bars -->
						<text x="114" y="69" font-size="9" text-anchor="middle" fill="#333">58%</text>
						<text x="146" y="93" font-size="9" text-anchor="middle" fill="#333">46%</text>
						<text x="178" y="75" font-size="9" text-anchor="middle" fill="#333">55%</text>
						<text x="254" y="119" font-size="9" text-anchor="middle" fill="#333">33%</text>
						<text x="286" y="147" font-size="9" text-anchor="middle" fill="#333">19%</text>
						<text x="318" y="139" font-size="9" text-anchor="middle" fill="#333">23%</text>
						<text x="394" y="129" font-size="9" text-anchor="middle" fill="#333">28%</text>
						<text x="426" y="141" font-size="9" text-anchor="middle" fill="#333">22%</text>
						<text x="458" y="135" font-size="9" text-anchor="middle" fill="#333">25%</text>
					</svg>
					<p style="text-align: center; font-size: 13px; color: #666; margin-top: 10px;">Titan-LLaMA recovers a substantial portion of the performance lost to segmentation across all domains.</p>
                    PubMedQA shows the strongest recovery, closing 75% of the gap between segmented-only and full attention (from 46% to 55%, vs. 58% full attention). CaseHOLD and AQUA-RAT show more modest but still meaningful improvements.
                </div>
                


            </div>
	    <div class="margin-right-block">
            Note that only Winogrande and BoolQ were properly hyperparameter-tuned; for the other tasks we only ran ~30 training steps, so their scores are likely under-optimized.
	    </div>
	</div>

    <!-- BACKGROUND divider -->
    <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <div class="section-divider"></div>
    </div>
    <div class="margin-right-block"></div>
    </div>

	<!-- LIMITATIONS -->
	<div class="content-margin-container" id="limitations">
			<div class="margin-left-block">
			</div>
	    <div class="main-content-block">
				<h2>Limitations and Future Work</h2>
				<b>1) Incomplete recovery.</b> Even with distillation, we don't fully recover full-attention performance on most benchmarks. The gap suggests that some information encoded in full attention is fundamentally difficult to compress into a learned memory module. Future work could explore larger NMM architectures or different distillation strategies.<br><br>

				<b>2) Throughput overhead.</b> Our current NMM implementation introduces computational overhead that negates the theoretical efficiency gains of segmented attention. As shown in Figure 1, the segmented+NMM model currently underperforms full attention in throughput. Optimized GPU and better memory management could address this.<br><br>

				<b>3) Limited hyperparameter tuning.</b> Due to compute and time constraints, only Winogrande and BoolQ were thoroughly tuned. Other benchmarks (CommonsenseQA, DROP, SQuAD) ran for ~30 training steps, so their scores are likely under-optimized. More extensive sweeps could improve results.<br><br>

				<b>4) Scale.</b> All experiments use LLaMA-3.1-8B. We would like to test whether our findings generalize to larger models where the capacity gap between NMM and full attention may be more or less pronounced.<br><br>

				<b>5) Future directions.</b> Promising next steps include: (1) training NMMs from distillation alone without LM loss, (2) exploring different NMM architectures beyond 2-layer MLPs, (3) investigating whether NMMs can enable genuine continual learning without catastrophic forgetting, and (4) scaling to longer sequences where the efficiency benefits of segmentation become more pronounced.
	    </div>
	    <div class="margin-right-block">
	    </div>
	</div>

    <!-- BACKGROUND divider -->
    <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <div class="section-divider"></div>
    </div>
    <div class="margin-right-block"></div>
    </div>

	<!-- REFERENCES -->
	<div class="content-margin-container" id="references">
			<div class="margin-left-block">
			</div>
	    <div class="main-content-block">
            <h2>References</h2>
					<div class='citation' style="height:auto"><br>
						<a id="ref_1"></a>[1] Hu et al. <a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a>, 2021<br><br>
						<a id="ref_2"></a>[2] Hewitt et al. <a href="https://arxiv.org/abs/2506.06266">Cartridges: Compact Representations for LLM Reasoning</a>, 2024<br><br>
						<a id="ref_3"></a>[3] Gu et al. <a href="https://arxiv.org/abs/2111.00396">Efficiently Modeling Long Sequences with Structured State Spaces</a>, 2021<br><br>
						<a id="ref_4"></a>[4] Behrouz et al. <a href="https://arxiv.org/abs/2501.00663">Titans: Learning to Memorize at Test Time</a>, Google Research, 2024<br><br>
						<a id="ref_5"></a>[5] <a href="https://huggingface.co/datasets/cerebras/SlimPajama-627B">SlimPajama-627B</a>, Cerebras, 2023<br><br>
						<a id="ref_6"></a>[6] <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">FineWeb-EDU</a>, HuggingFace, 2024<br><br>
						<a id="ref_7"></a>[7] Sakaguchi et al. <a href="https://arxiv.org/abs/1907.10641">WinoGrande: An Adversarial Winograd Schema Challenge at Scale</a>, 2019<br><br>
						<a id="ref_8"></a>[8] Clark et al. <a href="https://arxiv.org/abs/1905.10044">BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions</a>, 2019<br><br>
						<a id="ref_9"></a>[9] Talmor et al. <a href="https://arxiv.org/abs/1811.00937">CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge</a>, 2018<br><br>
						<a id="ref_10"></a>[10] Dua et al. <a href="https://arxiv.org/abs/1903.00161">DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning</a>, 2019<br><br>
						<a id="ref_11"></a>[11] Rajpurkar et al. <a href="https://arxiv.org/abs/1606.05250">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a>, 2016<br><br>
						<a id="ref_12"></a>[12] Jin et al. <a href="https://arxiv.org/abs/1909.06146">PubMedQA: A Dataset for Biomedical Research Question Answering</a>, 2019<br><br>
						<a id="ref_13"></a>[13] Chalkidis et al. <a href="https://arxiv.org/abs/2110.00976">LexGLUE: A Benchmark Dataset for Legal Language Understanding</a>, 2021<br><br>
						<a id="ref_14"></a>[14] Ling et al. <a href="https://github.com/google-deepmind/AQuA">AQUA-RAT: Algebra Question Answering Dataset</a>, 2017<br><br>
                        <a id="ref_15"></a>[15] Ali Behrouz, <a href="https://www.youtube.com/watch?v=ShYJc3Nm6QE">Cognitive Revolution podcast, YouTube</a>, 2025<br><br>
					
					</div>
	    </div>
	    <div class="margin-right-block">
	    </div>
	</div>

</body>

</html>
